<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Your first benchmark &#8212; MAPFbench documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=12dfc556" />
    <script src="../_static/documentation_options.js?v=2709fde1"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Installation" href="installation.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="your-first-benchmark">
<h1>Your first benchmark<a class="headerlink" href="#your-first-benchmark" title="Link to this heading">¶</a></h1>
<section id="the-benchmarking-workflow">
<h2>The benchmarking workflow<a class="headerlink" href="#the-benchmarking-workflow" title="Link to this heading">¶</a></h2>
<p>Running a benchmark in MAPFbench involves
the following steps:</p>
<ol class="arabic">
<li><p><strong>Importing a benchmark file</strong>. This file is a specially
formatted .yaml file containing a collection of tests.</p>
<p>Each test represents a MAPF problem instance and includes:</p>
<ul class="simple">
<li><p>A unique, identifying name</p></li>
<li><p>A graph</p></li>
<li><p>A list of entities which need to be considered during
the benchmark, that is agents, objectives and obstacles</p></li>
</ul>
<p>A benchmark may include more iterations of the same test.</p>
</li>
<li><p><strong>Running the benchmark</strong>. After having imported a benchmark, its tests must be
submitted to the running programs, and their results collected.</p>
<p>MAPFBench enables this by employing a client-side architectures:
there is a central server, the BenchmarkRunner, which
communicates with programs using a specific component,
called BenchmarkInspector.</p>
<p>Programs running MAPF solvers can request tests and submit
corresponding plans using this object. Moreover,
multiple instances of the same program can be run in
parallel, in order to finish all tests faster</p>
</li>
<li><p><strong>Calculating metrics</strong>. After having collected all test
results, MAPFbench elaborates them, determining
whether there are conflicts in the submitted plans and
calculating corresponding metrics (e.g. makespan, sum of costs…)</p></li>
<li><p><strong>Exporting results</strong>. Both the generated plans and the corresponding
metrics are exported in files: the former in .yaml format, the latter in a .csv table</p></li>
</ol>
</section>
<section id="instrumenting-a-program">
<h2>Instrumenting a program<a class="headerlink" href="#instrumenting-a-program" title="Link to this heading">¶</a></h2>
<p>In order to make a program able to communicate with the BenchmarkRunner, we
must instrument it.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>At the moment, it is possible to instruments scripts in Python only
(or, at least, with python bindings)</p>
</div>
<p>In the next section, we will try to make a program run a simple benchmark,
available in the <code class="docutils literal notranslate"><span class="pre">/examples/simple_benchmark.txt</span></code> folder</p>
<p>where the agent A1 needs to reach the objective T1</p>
<p>First of all, we need to create a BenchmarkInspector.
Inside the main code of the program:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">mapfbench</span>

<span class="n">benchmark_inspector</span> <span class="o">=</span> <span class="n">mapfbench</span><span class="o">.</span><span class="n">BenchmarkInspector</span><span class="p">()</span>
</pre></div>
</div>
<p>Then, we need to request a test from the BenchmarkRunner.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test</span> <span class="o">=</span> <span class="n">benchmark_inspector</span><span class="o">.</span><span class="n">request_random_test</span><span class="p">()</span>
</pre></div>
</div>
<p>Now we can access the information about the test.
The MAPFbench library uses its own internal format to
store test data, so you may need to convert it
to the one used by your program. For now, given the
simplicity of the test, we will skip this passage.</p>
<p>At this point, we wait for the program compute a plan.
Now we need to register the computed plan. Each plan is
composed of two main kinds actions: move and wait.
In order to register them, we write:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test</span><span class="o">.</span><span class="n">register_move</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;A1&quot;</span><span class="p">,</span> <span class="n">Node</span><span class="p">(</span><span class="n">coords</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)))</span>
<span class="c1"># or</span>
<span class="n">test</span><span class="o">.</span><span class="n">register_wait</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;A1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>where the parameters are:</p>
<ul class="simple">
<li><p>timestep at which the action is performed</p></li>
<li><p>The name of the agent performing it (“A1”)</p></li>
<li><p>In the case of the move action, we add a Node object,
representing the position in which the movement ends</p></li>
</ul>
<p>We don’t need to register the start position of the agents:
the inspector takes already care of that</p>
<p>Finally, after having registered all actions, we can submit the result</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">test</span><span class="o">.</span><span class="n">mark_as_solved</span><span class="p">()</span>
<span class="n">benchmark_inspector</span><span class="o">.</span><span class="n">submit_result</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="running-the-benchmark">
<h2>Running the benchmark<a class="headerlink" href="#running-the-benchmark" title="Link to this heading">¶</a></h2>
<p>Now we need to run the benchmark. In order to do this,
we use the library command line utility.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mapfbench<span class="w"> </span>run<span class="w"> </span>simple_benchmark.yaml
</pre></div>
</div>
<p>This command will start a BenchmarkRunner which will serve the tests
in the simple_benchmark.yaml file. By default, the BenchmarkRunner
communicates with the BenchmarkInspectors on localhost, port 9361.</p>
<p>Now we can run or MAPF program and wait for the results.
As soon as the results are received, in the same folder there will be
two new files:</p>
<ul class="simple">
<li><p>[timestamp]_SampleBenchmark_results.yaml, containing the plans computed by the program</p></li>
<li><p>[timestamp]_SampleBenchmark_metrics.csv, containing the metrics associated with the plan</p></li>
</ul>
<p>The benchmark run is now complete</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">MAPFbench</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Your first benchmark</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#the-benchmarking-workflow">The benchmarking workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#instrumenting-a-program">Instrumenting a program</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-the-benchmark">Running the benchmark</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="installation.html" title="previous chapter">Installation</a></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, Stefano Lanza, Luca Leonzio.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.3.7</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="../_sources/tutorial/introduction.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>